import re
import time
import pandas as pd
import requests
from bs4 import BeautifulSoup


URLS = {
    "College of Staten Island": "https://csidolphins.com/sports/mens-swimming-and-diving/roster",
    "York College": "https://yorkathletics.com/sports/mens-swimming-and-diving/roster",
    "Baruch College": "https://athletics.baruch.cuny.edu/sports/mens-swimming-and-diving/roster",
    "Brooklyn College": "https://www.brooklyncollegeathletics.com/sports/mens-swimming-and-diving/roster",
    "Lindenwood University": "https://lindenwoodlions.com/sports/mens-swimming-and-diving/roster",
    "Mckendree University": "https://mckbearcats.com/sports/mens-swimming-and-diving/roster",
    "Ramapo College": "https://ramapoathletics.com/sports/mens-swimming-and-diving/roster",
    "SUNY Oneota": "https://oneontaathletics.com/sports/mens-swimming-and-diving/roster",  
    "SUNY Binghamton": "https://bubearcats.com/sports/mens-swimming-and-diving/roster/2021-22",
    "Albright College": "https://albrightathletics.com/sports/mens-swimming-and-diving/roster/2021-22",
}


HDRS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
                  "(KHTML, like Gecko) Chrome/124.0 Safari/537.36"
}

HEIGHT_RE = re.compile(
    r"""(
        \b\d\s*'\s*\d{1,2}"?   # 6'1"  or  6 ' 1
        |\b\d-\d{1,2}\b        # 6-1
        |\b\d{3}\s*cm\b        # 185 cm
    )""", re.IGNORECASE | re.VERBOSE
)

def fetch(url, retries=2, sleep=1.0):
    last = None
    for _ in range(retries + 1):
        try:
            r = requests.get(url, headers=HDRS, timeout=30)
            r.raise_for_status()
            return r.text
        except Exception as e:
            last = e
            time.sleep(sleep)
    raise last

def clean_text(s):
    return re.sub(r"\s+", " ", s).strip() if s else None

def parse_height_from_text(s):
    if not s:
        return None
    m = HEIGHT_RE.search(s)
    return m.group(0) if m else None



def parse_sidearm(soup):
    rows = []

    cards = soup.select("li.sidearm-roster-player, tr.sidearm-roster-player, .sidearm-roster-player")
    if not cards:

        cards = soup.select("[class*=sidearm][class*=roster] .sidearm-roster-player, [class*=sidearm-roster-player]")

    for c in cards:
        name_el = (c.select_one(".sidearm-roster-player-name a")
                   or c.select_one(".sidearm-roster-player-name")
                   or c.select_one("[class*=roster-player-name]"))
        name = clean_text(name_el.get_text()) if name_el else None

        h_el = (c.select_one(".sidearm-roster-player-height")
                or c.select_one("[data-player-height]")
                or c.select_one("[class*=height]"))
        height = None
        if h_el:
            height = parse_height_from_text(clean_text(h_el.get_text()))

  
        if not height:
            for dt in c.select("dt, .sidearm-roster-player-attr-title"):
                label = clean_text(dt.get_text()).lower()
                if "ht" in label or "height" in label:
                    dd = dt.find_next_sibling("dd") or dt.parent.find("dd")
                    if dd:
                        height = parse_height_from_text(clean_text(dd.get_text()))
                        break

      
        if not height:
            height = parse_height_from_text(clean_text(c.get_text(" ")))

        if name:
            rows.append({"name": name, "height": height})
    return rows

def parse_prestosports(soup):
    rows = []

    cards = soup.select(".roster__player, .person-card, .roster-card")
    for c in cards:
        name_el = (c.select_one(".roster__name a, .roster__name, .person-card__name")
                   or c.select_one("h3 a, h3"))
        name = clean_text(name_el.get_text()) if name_el else None

        height = None
    
        for li in c.select("li, .roster__bio-item, .person-card__bio-item"):
            txt = clean_text(li.get_text(" "))
            if txt and ("height" in txt.lower() or "ht" in txt.lower()):
                height = parse_height_from_text(txt)
                if height:
                    break
        if not height:
            height = parse_height_from_text(clean_text(c.get_text(" ")))

        if name:
            rows.append({"name": name, "height": height})
    return rows

def detect_and_parse(html):
    soup = BeautifulSoup(html, "html.parser")

   
    meta_gen = (soup.find("meta", attrs={"name": "generator"}) or {}).get("content", "") .lower()
    classes = " ".join(set([c for t in soup.find_all(True, class_=True) for c in (t.get("class") or [])])).lower()

    if "sidearm" in meta_gen or "sidearm" in classes:
        rows = parse_sidearm(soup)
        if rows: return rows

    if "prestosports" in html.lower() or "roster__player" in classes or "person-card" in classes:
        rows = parse_prestosports(soup)
        if rows: return rows

    rows = parse_sidearm(soup)
    if rows: return rows
    rows = parse_prestosports(soup)
    return rows

all_rows = []

for school, url in URLS.items():
    try:
        html = fetch(url)
        rows = detect_and_parse(html)
       
        for r in rows:
            r["school"] = school
            r["source_url"] = url
        print(f"{school}: {len(rows)} players captured")
        all_rows.extend(rows)
    except Exception as e:
        print(f"{school}: ERROR -> {e}")

df = pd.DataFrame(all_rows, columns=["school", "name", "height", "source_url"])
df.to_csv("mens_swim_heights.csv", index=False)
print(f"\nSaved mens_swim_heights.csv with {len(df)} rows")
